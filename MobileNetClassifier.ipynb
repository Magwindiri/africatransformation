{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPSO9/NOBCORtvo9aFrKEZq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Magwindiri/africatransformation/blob/main/MobileNetClassifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FzSphw9kPOM2"
      },
      "outputs": [],
      "source": [
        "!pip install tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import pickle\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "from sklearn.metrics import recall_score, precision_score, accuracy_score, f1_score\n",
        "print(tf.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NqpchDxPQBzt",
        "outputId": "2c2ed97d-3ad5-4720-f7d5-49bc534d065d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.13.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"TF_FORCE_GPU_ALLOW_GROWTH\"]=\"true\""
      ],
      "metadata": {
        "id": "anQnXkaVUDYU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! export TF_FORCE_GPU_ALLOW_GROWTH=true"
      ],
      "metadata": {
        "id": "ABEm5kc2UdU6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "IMAGE_SIZE = (224,224,3)"
      ],
      "metadata": {
        "id": "rL-ZmbpoUjk7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The is configuring the GPU memory growth for TensorFlow to allocate memory on-demand, rather than allocating all available GPU memory upfront"
      ],
      "metadata": {
        "id": "hvUQx9MNXNn0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gpus_devices = tf.config.experimental.list_physical_devices('GPU')\n",
        "\n",
        "if gpus_devices:\n",
        "  for gpu in gpus_devices:\n",
        "    tf.config.experimental.set_memory_growth(gpu, True)"
      ],
      "metadata": {
        "id": "CpRrXtxsVO_i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use all available GPUs with MirroredStrategy\n",
        "strategyMirror = tf.distribute.MirroredStrategy()\n",
        "\n",
        "# Print the number of synchronized devices (usually the number of GPUs)\n",
        "print(f'Number of devices: {strategyMirror.num_replicas_in_sync}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZNNxlFkLX_Mj",
        "outputId": "9392c9cf-0ef5-4e51-f956-20c2e7e28064"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of devices: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# defines a convolutional neural network (CNN)\n",
        "\n",
        "def create_cnnnet(input_shape):\n",
        "    model = tf.keras.Sequential()\n",
        "\n",
        "    model.add(tf.keras.layers.Conv2D(64, (3, 3), input_shape=input_shape, padding='same', activation='relu'))\n",
        "    model.add(tf.keras.layers.BatchNormalization(momentum=0.9))\n",
        "    model.add(tf.keras.layers.MaxPooling2D())\n",
        "\n",
        "    model.add(tf.keras.layers.Conv2D(128, (3, 3), padding='same', activation='relu'))\n",
        "    model.add(tf.keras.layers.BatchNormalization(momentum=0.9))\n",
        "    model.add(tf.keras.layers.MaxPooling2D())\n",
        "\n",
        "    model.add(tf.keras.layers.Conv2D(256, (3, 3), padding='same', activation='relu'))\n",
        "    model.add(tf.keras.layers.BatchNormalization(momentum=0.9))\n",
        "\n",
        "    # Flatten the output\n",
        "    model.add(tf.keras.layers.GlobalMaxPooling2D())\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "xVyZMtugZj6z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# creates and returns a neural network model\n",
        "def second_model():\n",
        "    input_shape = (90, IMAGE_SIZE[0], IMAGE_SIZE[1], IMAGE_SIZE[2])\n",
        "    print('Input data shape: ', input_shape)\n",
        "\n",
        "    convnet = create_cnnnet(input_shape[1:])\n",
        "\n",
        "    model = tf.keras.Sequential()\n",
        "    model.add(tf.keras.layers.TimeDistributed(convnet, input_shape=input_shape))\n",
        "    model.add(tf.keras.layers.LSTM(64, return_sequences=False))\n",
        "\n",
        "    model.add(tf.keras.layers.Dense(256, activation='relu'))\n",
        "    model.add(tf.keras.layers.Dropout(0.5))\n",
        "    model.add(tf.keras.layers.Dense(128, activation='relu'))\n",
        "    model.add(tf.keras.layers.Dropout(0.5))\n",
        "    model.add(tf.keras.layers.Dense(64, activation='relu'))\n",
        "    model.add(tf.keras.layers.Dropout(0.5))\n",
        "    model.add(tf.keras.layers.Dense(16, activation='relu'))\n",
        "\n",
        "    model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "blB2r-cQaVcc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def first_model(IMAGE_SIZE):\n",
        "    # Define the input layer\n",
        "    input_shape = (None, IMAGE_SIZE[0], IMAGE_SIZE[1], IMAGE_SIZE[2])\n",
        "    #inp = tf.keras.layers.Input(shape=input_shape)\n",
        "    input = tf.keras.layers.Input(shape=input_shape)\n",
        "\n",
        "    # Load MobileNetV2 model with modifications\n",
        "    baseline_models = tf.keras.applications.MobileNetV2(\n",
        "        include_top=False,\n",
        "        weights='imagenet',\n",
        "        pooling='max',\n",
        "        classes=2,\n",
        "        input_shape=(IMAGE_SIZE[0], IMAGE_SIZE[1], 3),\n",
        "    )\n",
        "\n",
        "    # Freeze the layers of the pre-trained model\n",
        "    for layer in baseline_models.layers:\n",
        "        layer.trainable = False\n",
        "\n",
        "    # Wrap the MobileNetV2 with TimeDistributed layer for sequence input\n",
        "    x = tf.keras.layers.TimeDistributed(baseline_models)(input)\n",
        "\n",
        "    # Add an LSTM layer for sequence modeling\n",
        "    x = tf.keras.layers.LSTM(64, return_sequences=False)(x)\n",
        "\n",
        "    # Apply dropout to prevent overfitting\n",
        "    x = tf.keras.layers.Dropout(0.3)(x)\n",
        "\n",
        "    # Create the output layer for binary classification\n",
        "    out = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "    # Create the model\n",
        "    model = tf.keras.Model(inputs=input, outputs=out)\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(\n",
        "        loss=tf.keras.losses.BinaryCrossentropy(),\n",
        "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.01),\n",
        "        metrics=['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall()]\n",
        "    )\n",
        "\n",
        "    return model\n",
        "\n",
        "# Example usage:\n",
        "IMAGE_SIZE = (224, 224, 3)  # Define your IMAGE_SIZE\n",
        "model = first_model(IMAGE_SIZE)"
      ],
      "metadata": {
        "id": "RbLIQ2x0dVdE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_accs = []\n",
        "all_pres = []\n",
        "all_recs = []\n",
        "all_f1s = []"
      ],
      "metadata": {
        "id": "ZolALvuolSy8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "STARTING_FOLD_INDEX = 319"
      ],
      "metadata": {
        "id": "_EuLiBUYllky"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sklearn\n",
        "from scipy import interpolate\n",
        "import pickle, numpy as np\n",
        "from sklearn.metrics import roc_curve, roc_auc_score\n",
        "base_falsepositiverate = np.linspace(0, 1, 101)"
      ],
      "metadata": {
        "id": "faNhI5kprX-k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ROC curve analysis calculating the mean of true positive rates (calculate_mean_tpr) and the mean of area under the ROC curve (mean_auroc)"
      ],
      "metadata": {
        "id": "6YTl9GQcyFyK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_mean_tpr(TPRS, aurocs):\n",
        "\n",
        "    calculate_mean_tpr = np.mean(TPRS, axis=0)\n",
        "\n",
        "    # plot the auroc curves\n",
        "    calculate_mean_auroc = sum(aurocs) / len(aurocs)\n",
        "\n",
        "    return calculate_mean_tpr, calculate_mean_auroc"
      ],
      "metadata": {
        "id": "w9kd1fWGxIit"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IH7UtuPiIjmo",
        "outputId": "ed548b8b-8155-4313-f227-f1ed583a116f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import os\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "FOLD_TPRS = [] # all the saved TPRS\n",
        "FOLD_AUROCS = [] # all the saved AUROCs\n",
        "META_RESULT_MATRIX = [] # all the saved results\n",
        "\n",
        "#with open (\"file_names_folds.pkl\", 'rb') as f:\n",
        "#SEEDS, FOLD_FILES = pickle.load(f)\n",
        "with open (\"/content/drive/MyDrive/CPUT/PostGradDip/capstone/file_names_folds.pkl\", 'rb') as f:\n",
        "  SEEDS, FOLD_FILES = pickle.load(f)\n",
        "\n",
        "STARTING_FOLD_INDEX = 0\n",
        "\n",
        "index_to_start_at = STARTING_FOLD_INDEX\n",
        "\n",
        "TPRS, FPRS, local_aurocs = [],[], []\n",
        "for fold in FOLD_FILES[index_to_start_at:]:\n",
        "\n",
        "    print('FOLD::: ', fold)\n",
        "\n",
        "    train_files = [a.strip('_') for a in fold['train']]\n",
        "    test_files = [a.strip('_') for a in fold['test']]\n",
        "\n",
        "    X_train = []\n",
        "    X_test = []\n",
        "\n",
        "    y_train = []\n",
        "    y_test = []\n",
        "\n",
        "    for filename in train_files:\n",
        "        filename_int = int(filename.split('.mp4')[0])\n",
        "\n",
        "        if filename_int <= 115:\n",
        "            curr_y = 1\n",
        "            subdir_name = 'armflapping'\n",
        "        else:\n",
        "            curr_y = 0\n",
        "            subdir_name = 'control'\n",
        "\n",
        "        curr_x = []\n",
        "        for frame in os.listdir('behavior_data/' + subdir_name + '/' + filename):\n",
        "\n",
        "            frame_num = int(frame.split('.')[0])\n",
        "            if frame_num > 90:\n",
        "                continue\n",
        "\n",
        "            image = cv2.imread('behavior_data/' + subdir_name + '/' + filename + '/' + frame)\n",
        "            try:\n",
        "                image = image.reshape((image.shape[0], image.shape[1], image.shape[2]))\n",
        "            except:\n",
        "                continue\n",
        "\n",
        "            image = cv2.resize(image, (224, 224))\n",
        "            curr_x.append(image)\n",
        "\n",
        "        len_data = len(os.listdir('behavior_data/' + subdir_name + '/' + filename))\n",
        "        if len_data < 90:\n",
        "            for abc in range(len_data, 90):\n",
        "                curr_x.append(np.zeros((224, 224, 3)))\n",
        "\n",
        "        curr_x = np.array(curr_x)\n",
        "\n",
        "        X_train.append(curr_x)\n",
        "        y_train.append(curr_y)\n",
        "\n",
        "    for filename in test_files:\n",
        "        filename_int = int(filename.split('.mp4')[0])\n",
        "\n",
        "        if filename_int <= 115:\n",
        "            curr_y = 1\n",
        "            subdir_name = 'armflapping'\n",
        "        else:\n",
        "            curr_y = 0\n",
        "            subdir_name = 'control'\n",
        "\n",
        "        curr_x = []\n",
        "        for frame in os.listdir('behavior_data/' + subdir_name + '/' + filename):\n",
        "\n",
        "            frame_num = int(frame.split('.')[0])\n",
        "            if frame_num > 90:\n",
        "                continue\n",
        "\n",
        "            image = cv2.imread('behavior_data/' + subdir_name + '/' + filename + '/' + frame)\n",
        "            try:\n",
        "                image = image.reshape((image.shape[0], image.shape[1], image.shape[2]))\n",
        "            except:\n",
        "                continue\n",
        "\n",
        "            image = cv2.resize(image, (224, 224))\n",
        "            curr_x.append(image)\n",
        "\n",
        "        len_data = len(os.listdir('behavior_data/' + subdir_name + '/' + filename))\n",
        "        if len_data < 90:\n",
        "            for abc in range(len_data, 90):\n",
        "                curr_x.append(np.zeros((224, 224, 3)))\n",
        "\n",
        "        curr_x = np.array(curr_x)\n",
        "\n",
        "        X_test.append(curr_x)\n",
        "        y_test.append(curr_y)\n",
        "\n",
        "    X_train = np.array(X_train)\n",
        "    X_test = np.array(X_test)\n",
        "    y_train = np.array(y_train)\n",
        "    y_test = np.array(y_test)\n",
        "\n",
        "    print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
        "\n",
        "    model = first_model()\n",
        "\n",
        "    model.compile(loss = tf.keras.losses.BinaryCrossentropy(),\n",
        "                    optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
        "                    metrics = [['accuracy', tf.keras.metrics.Precision(name=\"precision\"), tf.keras.metrics.Recall(name=\"recall\")]])\n",
        "\n",
        "    history = model.fit(X_train,\n",
        "                        y_train,\n",
        "                        #validation_data = (X_test, y_test),\n",
        "                        batch_size = 16,\n",
        "                        epochs = 60)\n",
        "\n",
        "\n",
        "    ###\n",
        "    ### EVALUATE HERE!!!!!\n",
        "    ###\n",
        "\n",
        "    predictions = []\n",
        "    trues = []\n",
        "    for i in range(X_test.shape[0]):\n",
        "        X_to_predict = np.array([X_test[i]])\n",
        "        pred = model(X_to_predict)\n",
        "        print(pred)\n",
        "        true = y_test[i]\n",
        "        if pred < 0.5:\n",
        "            predictions.append(0)\n",
        "        else:\n",
        "            predictions.append(1)\n",
        "        trues.append(true)\n",
        "\n",
        "    acc = accuracy_score(trues, predictions)\n",
        "    pre = precision_score(trues, predictions)\n",
        "    rec = recall_score(trues, predictions)\n",
        "    f1 = f1_score(trues, predictions)\n",
        "\n",
        "    print('\\n\\n\\n\\n\\n\\n\\n  ', acc, pre, rec, f1, ' \\n\\n\\n\\n\\n\\n\\n')\n",
        "\n",
        "    all_accs.append(acc)\n",
        "    all_pres.append(pre)\n",
        "    all_recs.append(rec)\n",
        "    all_f1s.append(f1)\n",
        "\n",
        "    # get the training scores\n",
        "    training_accuracy = history.history['accuracy'][-1]\n",
        "    training_precision = history.history['precision'][-1]\n",
        "    training_recall = history.history['recall'][-1]\n",
        "    training_f1 = 2 * training_precision * training_recall / (training_precision + training_recall + tf.keras.backend.epsilon())\n",
        "\n",
        "    with open('performances/' + str(STARTING_FOLD_INDEX) + '_results.txt', 'w') as f:\n",
        "        f.write('Training Accuracy: ' + str(training_accuracy) + '\\n')\n",
        "        f.write('Training Precision: ' + str(training_precision) + '\\n')\n",
        "        f.write('Training Recall: ' + str(training_recall) + '\\n')\n",
        "        f.write('Training F1: ' + str(training_f1) + '\\n')\n",
        "        f.write('Validation Accuracy: ' + str(acc) + '\\n')\n",
        "        f.write('Validation Precision: ' + str(pre) + '\\n')\n",
        "        f.write('Validation Recall: ' + str(rec) + '\\n')\n",
        "        f.write('Validation F1: ' + str(f1) + '\\n')\n",
        "        f.write('AUROC: ' + str(roc_auc_score(trues, predictions)) + '\\n')\n",
        "        f.close()\n",
        "\n",
        "    STARTING_FOLD_INDEX += 1\n",
        "\n",
        "    fpr, tpr, _ = roc_curve(trues, predictions)\n",
        "    tpr = interp(base_fpr, fpr, tpr)\n",
        "    tpr[0] = 0.0\n",
        "\n",
        "    with open(\"tprs/\" + str(STARTING_FOLD_INDEX) + \"_tprs.pickle\", \"wb\") as f:\n",
        "        pickle.dump(tpr, f)"
      ],
      "metadata": {
        "id": "VhjHvTS7EMeK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}